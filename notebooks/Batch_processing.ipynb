{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "426e9e59-20e0-48d1-89f0-84eac4449d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/10/13 14:24:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySpark with MinIO, Delta, and Hive\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar,\"\n",
    "        \"/opt/spark/jars/s3-2.18.41.jar,\"\n",
    "        \"/opt/spark/jars/aws-java-sdk-1.12.367.jar,\"\n",
    "        \"/opt/spark/jars/delta-core_2.12-2.4.0.jar,\"\n",
    "        \"/opt/spark/jars/delta-storage-2.4.0.jar,\"\n",
    "        \"/opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar,\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://warehouse/\")\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3239fec0-dd84-4650-a455-b0c993b8341e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths for storing Delta tables in MinIO\n",
    "batch_business = \"s3a://deltalake/business\"\n",
    "batch_user = \"s3a://deltalake/user\"\n",
    "batch_check_in = \"s3a://deltalake/check_in\"\n",
    "batch_tip = \"s3a://deltalake/tip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1eba3dd-5c91-4808-934e-99783b027138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/13 15:16:25 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "24/10/13 15:16:53 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reading JSON data and saving as Delta tables\n",
    "businessDF = spark.read.json(\"s3a://data/yelp_academic_dataset_business.json\")\n",
    "businessDF.write.format(\"delta\").mode(\"overwrite\").save(batch_business)\n",
    "\n",
    "userDF = spark.read.json(\"s3a://data/yelp_academic_dataset_user.json\")\n",
    "userDF.write.format(\"delta\").mode(\"overwrite\").save(batch_user)\n",
    "\n",
    "check_inDF = spark.read.json(\"s3a://data/yelp_academic_dataset_checkin.json\")\n",
    "check_inDF.write.format(\"delta\").mode(\"overwrite\").save(batch_check_in)\n",
    "\n",
    "tipDF = spark.read.json(\"s3a://data/yelp_academic_dataset_tip.json\")\n",
    "tipDF.write.format(\"delta\").mode(\"overwrite\").save(batch_tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2866bcdb-5d6c-4347-83a7-6d8a9bd1489c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Delta tables in Hive Metastore using PySpark SQL\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS business\n",
    "    USING DELTA\n",
    "    LOCATION '{batch_business}'\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS user\n",
    "    USING DELTA\n",
    "    LOCATION '{batch_user}'\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS checkin\n",
    "    USING DELTA\n",
    "    LOCATION '{batch_check_in}'\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tip\n",
    "    USING DELTA\n",
    "    LOCATION '{batch_tip}'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a864819-a263-4b04-b861-0361e6647103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default| business|      false|\n",
      "|  default|  checkin|      false|\n",
      "|  default|      tip|      false|\n",
      "|  default|     user|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm tables are created by listing them\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab5c60ca-6f11-44a2-94ef-5f9d1ee854d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
