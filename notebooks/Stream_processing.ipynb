{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f51a6db-1d2b-4e12-9ca4-9814988fa3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/10/14 06:26:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/14 06:26:33 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9ede68cacac3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimized PySpark with MinIO, Delta, and Hive</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x76924aad5130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, from_json, row_number, year, window, max as max_, col, sum as sum_\n",
    "from pyspark.sql.types import DoubleType, LongType, StringType, StructField, StructType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Optimized PySpark with MinIO, Delta, and Hive\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/opt/spark/jars/hadoop-aws-3.3.4.jar,\"\n",
    "        \"/opt/spark/jars/s3-2.18.41.jar,\"\n",
    "        \"/opt/spark/jars/aws-java-sdk-1.12.367.jar,\"\n",
    "        \"/opt/spark/jars/delta-core_2.12-2.4.0.jar,\"\n",
    "        \"/opt/spark/jars/delta-storage-2.4.0.jar,\"\n",
    "        \"/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.4.0.jar,\"\n",
    "        \"/opt/spark/jars/kafka-clients-3.3.2.jar,\"\n",
    "        \"/opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar,\"\n",
    "        \"/opt/spark/jars/spark-streaming-kafka-0-10-assembly_2.12-3.4.0.jar,\"\n",
    "        \"/opt/spark/jars/commons-pool2-2.11.1.jar,\"\n",
    "        \"/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.4.0.jar\",\n",
    "    )\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://warehouse/\")\n",
    "    .config(\"spark.eventLog.enabled\", \"false\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd14902-5d90-4de6-b32b-a8dd99fd929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default| business|      false|\n",
      "|  default|  checkin|      false|\n",
      "|  default|   review|      false|\n",
      "|  default|      tip|      false|\n",
      "|  default|     user|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bcd97cf-ffd2-492d-b182-4b064e4fe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka specific configurations for optimization (single broker, 1 partition, RF = 1)\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": \"kafka:9092\",  # Kafka broker\n",
    "    \"subscribe\": \"delta-pipeline\",  # Kafka topic\n",
    "    \"startingOffsets\": \"latest\",  # Start consuming from the latest offset\n",
    "    \"maxOffsetsPerTrigger\": \"500\",  # Maximum number of messages per batch (adjusted for small partitions)\n",
    "    \"minPartitions\": \"1\",  # Single partition\n",
    "    \"failOnDataLoss\": \"false\",  # Handle data loss gracefully\n",
    "}\n",
    "\n",
    "\n",
    "# Helper function to generate random checkpoint directory in MinIO\n",
    "def random_checkpoint_dir():\n",
    "    return f\"s3a://deltalake/checkpoints/{''.join(random.choices(string.ascii_lowercase + string.digits, k=10))}\"\n",
    "\n",
    "\n",
    "# Stop all active streaming queries\n",
    "def stop_all_streams():\n",
    "    for s in spark.streams.active:\n",
    "        print(f\"Stopping {s.name}\")\n",
    "        s.stop()\n",
    "\n",
    "\n",
    "# Bronze layer ingestion with optimized Kafka configurations\n",
    "def bronze_ingest_data(bronze_path):\n",
    "    \"\"\"\n",
    "    Function to start a streaming query with a stream of data from Kafka and append to the Delta table.\n",
    "    \"\"\"\n",
    "    # Define schema for the JSON data coming from Kafka\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"review_id\", StringType(), True),\n",
    "            StructField(\"user_id\", StringType(), True),\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"stars\", DoubleType(), True),\n",
    "            StructField(\"useful\", LongType(), True),\n",
    "            StructField(\"funny\", LongType(), True),\n",
    "            StructField(\"cool\", LongType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Read data from Kafka using the optimized configurations\n",
    "    review_df = spark.readStream.format(\"kafka\").options(**kafka_options).load()\n",
    "\n",
    "    # Deserialize JSON data\n",
    "    review_df_parsed = (\n",
    "        review_df.select(\n",
    "            from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"), \"timestamp\"\n",
    "        )\n",
    "        .select(\"data.*\", \"timestamp\")\n",
    "        .withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "    )\n",
    "\n",
    "    # Write to Delta table (bronze layer)\n",
    "    checkpoint_dir = random_checkpoint_dir()\n",
    "    review_bronze = (\n",
    "        review_df_parsed.writeStream.format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir)\n",
    "        .start(bronze_path)\n",
    "    )\n",
    "\n",
    "    return review_bronze\n",
    "\n",
    "def silver_process(bronze_path, silver_path):\n",
    "    \"\"\"\n",
    "    Read data from the bronze table, process null data, and save into the silver table.\n",
    "    \"\"\"\n",
    "    # Read data from the bronze Delta table\n",
    "    read_reviewBronze = spark.readStream \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .load(bronze_path)\n",
    "    \n",
    "    checkpointDir = random_checkpoint_dir()\n",
    "\n",
    "    # Select relevant columns from the bronze layer\n",
    "    ratings = read_reviewBronze.select(\n",
    "        \"user_id\", \"business_id\", \"stars\", \"date\", \"cool\", \"funny\", \"useful\", \"timestamp\"\n",
    "    )\n",
    "    \n",
    "    # Handle null values by filling in defaults or dropping rows\n",
    "    ratings_cleaned = ratings.select(\n",
    "        col(\"user_id\"),\n",
    "        col(\"business_id\"),\n",
    "        when(col(\"stars\").isNull(), 0).otherwise(col(\"stars\")).alias(\"stars\"),  # Fill null stars with 0\n",
    "        when(col(\"date\").isNull(), \"1970-01-01\").otherwise(col(\"date\")).alias(\"date\"),  # Fill null date with default\n",
    "        when(col(\"cool\").isNull(), 0).otherwise(col(\"cool\")).alias(\"cool\"),  # Fill null cool with 0\n",
    "        when(col(\"funny\").isNull(), 0).otherwise(col(\"funny\")).alias(\"funny\"),  # Fill null funny with 0\n",
    "        when(col(\"useful\").isNull(), 0).otherwise(col(\"useful\")).alias(\"useful\"),  # Fill null useful with 0\n",
    "        col(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    # Write the cleaned data to the silver Delta table\n",
    "    write_reviewSilver = ratings_cleaned \\\n",
    "                            .writeStream \\\n",
    "                            .format(\"delta\") \\\n",
    "                            .outputMode(\"append\") \\\n",
    "                            .option(\"checkpointLocation\", checkpointDir) \\\n",
    "                            .start(silver_path)\n",
    "    \n",
    "    return write_reviewSilver\n",
    "\n",
    "\n",
    "def gold_process(silver_path, gold_path):\n",
    "    \"\"\"\n",
    "    Function to read data from the silver table and produce aggregated datasets for business insights.\n",
    "    \"\"\"\n",
    "    # Read data from the silver Delta table (streaming)\n",
    "    review_silver = spark.readStream.format(\"delta\").load(silver_path)\n",
    "\n",
    "    # Add the year column to the review data\n",
    "    review_silver_with_year = review_silver.withColumn(\"year\", year(col(\"date\")))\n",
    "\n",
    "    # Read business and user tables from Hive (assumed to be registered in Hive)\n",
    "    business = spark.table(\"business\")  # Replace 'business' with the actual Hive table name\n",
    "    user = spark.table(\"user\")  # Replace 'user' with the actual Hive table name\n",
    "\n",
    "    # Alias the tables to avoid column name collisions\n",
    "    review_silver_with_year = review_silver_with_year.alias(\"reviews\")\n",
    "    business = business.alias(\"business\")\n",
    "    user = user.alias(\"user\")\n",
    "\n",
    "    # Join business and user data with review data (correcting column names)\n",
    "    join_df = review_silver_with_year \\\n",
    "        .join(business, review_silver_with_year[\"business_id\"] == business[\"business_id\"], \"left\") \\\n",
    "        .join(user, review_silver_with_year[\"user_id\"] == user[\"user_id\"], \"left\")\n",
    "\n",
    "    # Select the required columns for the final output (specifying source table for ambiguous columns)\n",
    "    result_df = join_df.select(\n",
    "        review_silver_with_year[\"user_id\"],\n",
    "        review_silver_with_year[\"business_id\"],\n",
    "        review_silver_with_year[\"stars\"].alias(\"review_stars\"),  # Use the stars from reviews table\n",
    "        review_silver_with_year[\"date\"],\n",
    "        review_silver_with_year[\"cool\"],\n",
    "        review_silver_with_year[\"funny\"],\n",
    "        review_silver_with_year[\"useful\"],\n",
    "        review_silver_with_year[\"year\"],\n",
    "        business[\"address\"],\n",
    "        business[\"city\"],\n",
    "        business[\"state\"],\n",
    "        business[\"name\"].alias(\"business_name\"),  # Use the business name from business table\n",
    "        user[\"name\"].alias(\"user_name\"),  # Use the user name from user table\n",
    "        review_silver_with_year[\"timestamp\"],\n",
    "    )\n",
    "\n",
    "    # Write the processed data to the gold Delta table (streaming)\n",
    "    checkpoint_dir = random_checkpoint_dir()\n",
    "    review_gold = result_df \\\n",
    "        .writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "        .start(gold_path)\n",
    "\n",
    "    return review_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f27b95a-a198-4a0b-825b-c743889721db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for Delta table storage\n",
    "bronze_path = \"s3a://deltalake/bronze\"\n",
    "silver_path = \"s3a://deltalake/silver\"\n",
    "gold_path = \"s3a://deltalake/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bfd025-cd72-42aa-9ddd-3e39cdd02bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 06:27:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/10/14 06:27:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "24/10/14 06:27:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bronze_query = bronze_ingest_data(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7531adc-c2b1-4053-9c2d-c7ea9c25af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 06:29:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "[Stage 36:=====================>                                  (29 + 5) / 74]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "silver_query = silver_process(bronze_path, silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9aa17fb-7bbb-44e5-92f8-8da062e2ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 06:29:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "[Stage 337:>               (0 + 4) / 25][Stage 339:>                (0 + 0) / 4]\r"
     ]
    }
   ],
   "source": [
    "gold_query = gold_process(silver_path, gold_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65231c3f-5fa3-4b5a-80b6-8e6b6cb97cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Await termination for the streaming queries\n",
    "bronze_query.awaitTermination()\n",
    "silver_query.awaitTermination()\n",
    "gold_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced473d-03e7-4f14-9be0-7bfed3bd67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14befa13-cf01-495f-8008-3b7e1c30ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
